// The Hascal Lexer
//
// Copyright (c) 2019-2022 Hascal Foundation
// all rights reserved.

use strings
use keywords
use token
use conv

var _lexme = "" // current lexeme
var current = -1 // current character
var _source = "" // source code
var line = 1 // current line

var _tokens : [Token]?

// check if current character is at the end of the source code
function isAtEnd() : bool {
    if current >= len(_source) {
        return true
    }
    return false
}

// get next character
function advance() : char {
    current = current + 1
    return _source[current]
}

// get next character without advancing
function peek() : char {
    if isAtEnd() {
        return '\0'
    }
    return _source[current + 1]
}

// lexer error
function lexer_error(filename:string, msg:string,line:int) {
    var fmt_msg = format("{}: Illegal character '{}':{}", filename, msg, to_string(line))
}

// scan next token
function scan_Token(filename:string){
    var c = advance()
    var tok : Token?

    // skip whitespace
    if c == ' ' {
        scan_Token(filename)
    } 
    // skip newline(lines saved as a token, we need it in parser phase)
    else if c == '\n' {
        tok = Token("NEWLINE", "", line)
        append(_tokens,tok)
        line = line + 1
    } 

    else if c == '(' {
        tok = Token("LPAREN","(",line)
        append(_tokens,tok)
    } else if c == ')' {
        tok = Token("RPAREN",")",line)
        append(_tokens,tok)
    } 
    
    else if is_alpha(c) or c == '_' {
        _lexme = to_string(c)
        c = advance()
        while (isAtEnd() == false) and (is_alpha(c) or is_number(c) or c == '_') {
            _lexme = _lexme + to_string(c)
            c = advance()
        }

        if check_keyword(_lexme) {
            tok = Token(_lexme,_lexme,line)
        } else {
            tok = Token("NAME",_lexme,line)
        }
        append(_tokens,tok)
    } 

    else if is_number(c) {
        _lexme = to_string(c)
        while not isAtEnd() and is_number(c) {
            c = advance()
            _lexme = _lexme + to_string(c) 
        }
        tok = Token("NUMBER", _lexme, line)
        append(_tokens,tok)
    } 
    else if c == '"' {
        _lexme = to_string(c)
        while not isAtEnd() and c != '"' {
            _lexme = _lexme + to_string(c)
            c = advance()
        }
        c = advance()
        tok = Token("STRING", _lexme, line)
        append(_tokens,tok)
    } 
    
    else if c == '+' {
        tok = Token("PLUS", "+", line)
        append(_tokens,tok)
    } 
    else if c == '-' {
        tok = Token("MINUS", "-", line)
        append(_tokens,tok)
    } 
    else if c == '*' {
        tok = Token("TIMES", "*", line)
        append(_tokens,tok)
    } 
    else if c == '/' {
        c = advance()
        // skip single line comments
        if c == '/' {
            while not isAtEnd() and c != '\n' {
                c = advance()
            }
        }
        // skip multi line comments
        else if c == '*' {
            while not isAtEnd() and c != '*' and c != '/' {
                c = advance()
            }
            c = advance()
        }
        // else it's a division
        else {
            tok = Token("DIV", "/", line)
            append(_tokens,tok)
        }
    }

    else if c == '=' {
        c = peek()

        // check if it's a assign or equal
        if c == '=' {
            tok = Token("EQUAL", "==", line)
            append(_tokens,tok)
        } else {
            tok = Token("ASSIGN","=",line)
            append(_tokens,tok)
        }
    }
    else if c == '<' {
        c = peek()

        // check if it's a less than or less than equal
        if c == '=' {
            tok = Token("LESSEQ", "<=", line)
            append(_tokens,tok)
        } else {
            tok = Token("LESS","<",line)
            append(_tokens,tok)
        }
    }
    else if c == '>' {
        c = peek()

        // check if it's a greater than or greater than equal
        if c == '=' {
            tok = Token("GREATEREQ", ">=", line)
            append(_tokens,tok)
        } else {
            tok = Token("GREATER",">",line)
            append(_tokens,tok)
        }
    }
    else if c == '!' {
        c = peek()

        // check if it's a greater than or greater than equal
        if c == '=' {
            tok = Token("GREATEREQ", ">=", line)
            append(_tokens,tok)
        } else {
            lexer_error(filename,to_string(c),line)
            exit(1)
        }
    }

    else if c == '.' {
        tok = Token("DOT", ".", line)
        append(_tokens,tok)
    } 
    else if c == ':' {
        tok = Token("COLON", ":", line)
        append(_tokens,tok)
    }
    else if c == ',' {
        tok = Token("COMMA", ",", line)
        append(_tokens,tok)
    }

    else if c == '{' {
        tok = Token("LBC", "{", line)
        append(_tokens,tok)
    }
    else if c == '}' {
        tok = Token("RBC", "}", line)
        append(_tokens,tok)
    }

    else if c == '[' {
        tok = Token("LBRCK", "[", line)
        append(_tokens,tok)
    }
    else if c == ']' {
        tok = Token("RBRCK", "]", line)
        append(_tokens,tok)
    }
    
}
function tokenizer(filename:string,src:string): [Token] {
    _source = src
    while isAtEnd() == false {
        scan_Token(filename)
    }
    return _tokens
}
